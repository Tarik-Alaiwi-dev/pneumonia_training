{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUAL TRANSFORMER TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/pneumonia_data/vit\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/pneumonia_data/vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "\n",
    "from utils.vit_2 import ViTClassifier, PneumoniaDataset\n",
    "from utils.vit_utils import VitUtilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Classifier Initialized!\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ViTClassifier().to(device)\n",
    "\n",
    "print(\"ViT Classifier Initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 11888, Validation Samples: 2972\n"
     ]
    }
   ],
   "source": [
    "CLASS_NAMES = {\"pneumonia\": 0, \"normal\": 1}\n",
    "\n",
    "# train_dir = r\"D:\\pulpit\\wbudowane\\pneumonia_training\\data\\train\"\n",
    "# val_dir = r\"D:\\pulpit\\wbudowane\\pneumonia_training\\data\\val\"\n",
    "train_dir = \"/content/drive/MyDrive/pneumonia_data/data/clsif/train\"\n",
    "val_dir = \"/content/drive/MyDrive/pneumonia_data/data/clsif/val\"\n",
    "\n",
    "train_dataset = PneumoniaDataset(train_dir, CLASS_NAMES)\n",
    "val_dataset = PneumoniaDataset(val_dir, CLASS_NAMES)\n",
    "\n",
    "print(f\"Training Samples: {len(train_dataset)}, Validation Samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders Created Successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, sampler=RandomSampler(train_dataset), num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders Created Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load The Model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"vit_pneumonia_classifier_best.pth\", map_location=torch.device('cpu')))\n",
    "# model.to(device)  \n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Criterion and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW([\n",
    "        {'params': model.vit.patch_embed.parameters(), 'lr': 1e-6},  # Frozen in practice\n",
    "        {'params': model.vit.blocks[:-6].parameters(), 'lr': 3e-5},  # Early layers\n",
    "        {'params': model.vit.blocks[-6:-3].parameters(), 'lr': 1e-4}, # Mid layers\n",
    "        {'params': model.vit.blocks[-3:].parameters(), 'lr': 3e-4},   # Late layers\n",
    "        {'params': model.vit.head.parameters(), 'lr': 1e-3}           # Classification head\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 1486/1486 [10:43<00:00,  2.31it/s, loss=0.0380]\n",
      "Epoch 2/10 [Train]: 100%|██████████| 1486/1486 [10:38<00:00,  2.33it/s, loss=0.0045]\n",
      "Epoch 3/10 [Train]: 100%|██████████| 1486/1486 [10:31<00:00,  2.35it/s, loss=0.3985]\n",
      "Epoch 4/10 [Train]: 100%|██████████| 1486/1486 [10:41<00:00,  2.32it/s, loss=0.4600]\n",
      "Epoch 5/10 [Train]: 100%|██████████| 1486/1486 [10:47<00:00,  2.30it/s, loss=0.0028]\n",
      "Epoch 6/10 [Train]: 100%|██████████| 1486/1486 [10:22<00:00,  2.39it/s, loss=0.0129]\n",
      "Epoch 7/10 [Train]: 100%|██████████| 1486/1486 [10:18<00:00,  2.40it/s, loss=0.0017]\n",
      "Epoch 8/10 [Train]: 100%|██████████| 1486/1486 [10:32<00:00,  2.35it/s, loss=0.4860]\n",
      "Epoch 9/10 [Train]: 100%|██████████| 1486/1486 [10:30<00:00,  2.36it/s, loss=0.0024]\n",
      "Epoch 10/10 [Train]: 100%|██████████| 1486/1486 [10:19<00:00,  2.40it/s, loss=0.0859]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n",
      "Final Confusion Matrix:\n",
      "[[1119   83]\n",
      " [ 123 1647]]\n",
      "Model Saved Successfully!\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "VitUtilities.train_vit(model, train_loader, val_loader, optimizer, criterion, device, epochs)\n",
    "torch.save(model.state_dict(), \"/content/drive/MyDrive/pneumonia_data/data/vit_pneumonia_classifier_test.pth\")\n",
    "\n",
    "print(\"Model Saved Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def predict_vit(model, image_path, device):\n",
    "    \"\"\"\n",
    "    Function to predict the class of an image using a trained ViT model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained ViT model.\n",
    "        image_path (str): Path to the image file.\n",
    "        device (torch.device): Device to run inference on (CPU or CUDA).\n",
    "\n",
    "    Returns:\n",
    "        str: Predicted class label (\"Pneumonia\" or \"Normal\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the same transformations as training\n",
    "    transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.repeat(3, 1, 1)),  # Repeat to 3 channels\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "            transforms.Resize(224),\n",
    "            transforms.CenterCrop(224)\n",
    "        ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"L\")\n",
    "    image = VitUtilities.apply_clahe(image)\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Add batch dimensi\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor).squeeze()\n",
    "        probability = torch.sigmoid(output).item()  # Convert logits to probability\n",
    "\n",
    "    # Class mapping (assuming binary classification)\n",
    "    predicted_class = \"Pneumonia\" if probability < 0.5 else \"Normal\"\n",
    "\n",
    "    return predicted_class, probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Normal', 0.9977536797523499)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_path = r\"/content/drive/MyDrive/pneumonia_data/data/clsif/rr3.png\"\n",
    "\n",
    "predict_vit(model, image_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "def vit_gradcam(model, image_path, device, block_idx=-1):\n",
    "    \"\"\"\n",
    "    Working Grad-CAM for modern timm ViTs\n",
    "    Args:\n",
    "        block_idx: Which transformer block to visualize (-1 for last)\n",
    "    \"\"\"\n",
    "    # Preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3, 1, 1)),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3),\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224)\n",
    "    ])\n",
    "    \n",
    "    # Load and process image\n",
    "    orig_img = Image.open(image_path).convert(\"L\")\n",
    "    img_clahe = VitUtilities.apply_clahe(orig_img)\n",
    "    img_tensor = transform(img_clahe).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Hook setup\n",
    "    attention_weights = []\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        # For timm's Attention module, we need to manually compute attention\n",
    "        B, N, C = input[0].shape\n",
    "        qkv = module.qkv(input[0]).reshape(B, N, 3, module.num_heads, C // module.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # [B, heads, N, C//heads]\n",
    "        \n",
    "        # Compute attention matrix\n",
    "        attn = (q @ k.transpose(-2, -1)) * module.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attention_weights.append(attn.detach().mean(dim=1))  # Average heads\n",
    "    \n",
    "    # Register hook\n",
    "    target_block = model.vit.blocks[block_idx]\n",
    "    handle = target_block.attn.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        prob = torch.sigmoid(output).item()\n",
    "        pred_class = \"Pneumonia\" if prob > 0.5 else \"Normal\"\n",
    "        \n",
    "        if not attention_weights:\n",
    "            raise ValueError(\"No attention weights captured. Check model architecture.\")\n",
    "        \n",
    "        # Process attention weights\n",
    "        attn = attention_weights[0][0]  # [N+1, N+1]\n",
    "        attn = attn[1:, 1:]  # Remove CLS token\n",
    "        cam = attn.mean(dim=0)  # Average over keys\n",
    "        \n",
    "        # Reshape to patch grid\n",
    "        grid_size = int(np.sqrt(cam.shape[0]))\n",
    "        cam = cam.reshape(grid_size, grid_size)\n",
    "        \n",
    "        # Interpolate to image size\n",
    "        cam = F.interpolate(cam.unsqueeze(0).unsqueeze(0), \n",
    "                          size=(224, 224), \n",
    "                          mode='bicubic').squeeze().cpu().numpy()\n",
    "        \n",
    "        # Normalize\n",
    "        cam = np.maximum(cam, 0)\n",
    "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
    "        \n",
    "        # Create overlay\n",
    "        img_np = np.array(orig_img.convert(\"RGB\"))\n",
    "        img_np = np.array(transforms.functional.resize(Image.fromarray(img_np), (224, 224)))\n",
    "        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "        overlayed = cv2.addWeighted(img_np, 0.6, heatmap, 0.4, 0)\n",
    "    \n",
    "    # Remove hook\n",
    "    handle.remove()\n",
    "    \n",
    "    return overlayed, pred_class, prob\n",
    "\n",
    "def plot_gradcam(gradcam_result):\n",
    "    \"\"\"Visualize Grad-CAM results\"\"\"\n",
    "    overlayed, pred_class, prob = gradcam_result\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(overlayed)\n",
    "    plt.title(f\"Pred: {pred_class} ({prob:.2f})\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(overlayed)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"/content/drive/MyDrive/pneumonia_data/data/clsif/rr3.png\"\n",
    "result = vit_gradcam(model, image_path, device, block_idx=3)\n",
    "plot_gradcam(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
